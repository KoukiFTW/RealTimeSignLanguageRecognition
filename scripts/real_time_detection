import cv2
import numpy as np
import tensorflow as tf
import mediapipe as mp

# Load the trained model
model_path = "models/sign_language_model_final.h5"
model = tf.keras.models.load_model(model_path)
print("Model loaded successfully!")

# Class labels (update these with your dataset's labels)
class_labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']

# Initialize MediaPipe Hands
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False,
                       max_num_hands=1,
                       min_detection_confidence=0.5,
                       min_tracking_confidence=0.5)

# Function to preprocess the ROI for model prediction
def preprocess_frame(frame, target_size=(224, 224)):
    resized_frame = cv2.resize(frame, target_size)
    array_frame = np.array(resized_frame, dtype="float32") / 255.0
    return np.expand_dims(array_frame, axis=0)

# Start the webcam
cap = cv2.VideoCapture(1)

if not cap.isOpened():
    print("Error: Could not open webcam.")
    exit()

print("Press 'q' to exit.")

while True:
    ret, frame = cap.read()
    if not ret:
        print("Failed to grab frame. Exiting...")
        break

    # Flip the frame horizontally for a mirrored view
    frame = cv2.flip(frame, 1)
    h, w, c = frame.shape

    # Convert the frame to RGB for MediaPipe
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Process the frame with MediaPipe Hands
    result = hands.process(rgb_frame)

    if result.multi_hand_landmarks:
        for hand_landmarks in result.multi_hand_landmarks:
            # Get the bounding box coordinates of the hand
            x_min = int(min([lm.x for lm in hand_landmarks.landmark]) * w)
            y_min = int(min([lm.y for lm in hand_landmarks.landmark]) * h)
            x_max = int(max([lm.x for lm in hand_landmarks.landmark]) * w)
            y_max = int(max([lm.y for lm in hand_landmarks.landmark]) * h)

            # Expand the bounding box slightly
            padding = 20
            x_min = max(0, x_min - padding)
            y_min = max(0, y_min - padding)
            x_max = min(w, x_max + padding)
            y_max = min(h, y_max + padding)

            # Extract the region of interest (ROI)
            roi = frame[y_min:y_max, x_min:x_max]

            if roi.size > 0:
                # Preprocess the ROI
                processed_roi = preprocess_frame(roi)

                # Predict the gesture
                predictions = model.predict(processed_roi)
                predicted_label = class_labels[np.argmax(predictions)]

                # Draw the bounding box and label on the frame
                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)
                cv2.putText(frame, f"Prediction: {predicted_label}", (x_min, y_min - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)

    # Display the frame
    cv2.imshow("Sign Language Recognition", frame)

    # Exit on pressing 'q'
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release resources
cap.release()
cv2.destroyAllWindows()
hands.close()