import cv2
import numpy as np
import mediapipe as mp
import tensorflow as tf

# Load the trained model
model = tf.keras.models.load_model("models/landmark_model_final.h5")
print("Landmark-based model loaded successfully!")

# Class labels (update based on your dataset)
class_labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']

# Initialize MediaPipe Hands
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)

def extract_landmarks_and_box(frame):
    """
    Extract hand landmarks and bounding box from the frame using MediaPipe Hands.
    :param frame: Input frame (numpy array).
    :return: Tuple (landmarks, bounding_box) or (None, None) if no hand is detected.
    """
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    result = hands.process(frame_rgb)

    if result.multi_hand_landmarks:
        for hand_landmarks in result.multi_hand_landmarks:
            # Extract x, y, z coordinates for all 21 landmarks
            landmarks = []
            for lm in hand_landmarks.landmark:
                landmarks.extend([lm.x, lm.y, lm.z])

            # Calculate bounding box
            h, w, _ = frame.shape
            x_min = int(min([lm.x for lm in hand_landmarks.landmark]) * w)
            y_min = int(min([lm.y for lm in hand_landmarks.landmark]) * h)
            x_max = int(max([lm.x for lm in hand_landmarks.landmark]) * w)
            y_max = int(max([lm.y for lm in hand_landmarks.landmark]) * h)

            # Add padding to the bounding box
            padding = 20
            x_min = max(0, x_min - padding)
            y_min = max(0, y_min - padding)
            x_max = min(w, x_max + padding)
            y_max = min(h, y_max + padding)

            bounding_box = (x_min, y_min, x_max, y_max)

            return np.array(landmarks), bounding_box

    return None, None

# Start the webcam
cap = cv2.VideoCapture(1)

if not cap.isOpened():
    print("Error: Could not open webcam.")
    exit()

print("Press 'q' to exit.")

while True:
    ret, frame = cap.read()
    if not ret:
        print("Failed to grab frame. Exiting...")
        break

    # Flip the frame horizontally for a mirrored view
    frame = cv2.flip(frame, 1)

    # Extract landmarks and bounding box
    landmarks, bounding_box = extract_landmarks_and_box(frame)

    if landmarks is not None and bounding_box is not None:
        # Reshape landmarks for prediction (1, 63)
        landmarks_input = landmarks.reshape(1, -1)

        # Predict the gesture
        predictions = model.predict(landmarks_input)
        predicted_label = class_labels[np.argmax(predictions)]

        # Draw the bounding box
        x_min, y_min, x_max, y_max = bounding_box
        cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)

        # Overlay the prediction on the frame
        cv2.putText(frame, f"Prediction: {predicted_label}", (x_min, y_min - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)

    # Display the frame
    cv2.imshow("Sign Language Recognition (Landmark-Based)", frame)

    # Exit on pressing 'q'
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release resources
cap.release()
cv2.destroyAllWindows()
hands.close()